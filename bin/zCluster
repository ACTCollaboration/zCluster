#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
    Copyright 2017 Matt Hilton (matt.hilton@mykolab.com)
    
    This file is part of zCluster.

    zCluster is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    zCluster is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with zCluster.  If not, see <http://www.gnu.org/licenses/>.

"""

import os
import sys
import argparse
import glob
import zCluster
from zCluster import *
import sqlite3
import pylab as plt
import pickle
import numpy as np
from astLib import *
from scipy import interpolate
import astropy.io.fits as pyfits
import astropy.table as atpy
import datetime
import urllib.request
#import multiprocessing as mp
import IPython
plt.matplotlib.interactive(False)

#------------------------------------------------------------------------------------------------------------
def update_rcParams(dict={}):
    """
    Based on Cristobal's preferred settings.
    
    """
    default = {}
    for tick in ('xtick', 'ytick'):
        default['{0}.major.size'.format(tick)] = 8
        default['{0}.minor.size'.format(tick)] = 4
        default['{0}.major.width'.format(tick)] = 2
        default['{0}.minor.width'.format(tick)] = 2
        default['{0}.labelsize'.format(tick)] = 20
        default['{0}.direction'.format(tick)] = 'in'
    default['xtick.top'] = True
    default['ytick.right'] = True
    default['axes.linewidth'] = 2
    default['axes.labelsize'] = 22
    default['font.size'] = 22
    default['font.family']='sans-serif'
    default['legend.fontsize'] = 18
    default['lines.linewidth'] = 2

    for key in default:
        plt.rcParams[key] = default[key]
    # if any parameters are specified, overwrite anything previously
    # defined
    for key in dict:
        plt.rcParams[key] = dict[key]
        
#-------------------------------------------------------------------------------------------------------------
def parseClusterCatalog(fileName):
    """Parses .fits or .csv table with name, RADeg, decDeg columns into dictionary list.
    
    """
    
    if fileName.split(".")[-1] == "csv":
        tab=atpy.Table().read(fileName, format = 'ascii')
    else:
        tab=atpy.Table().read(fileName)
    catalog=[]
    wantedKeys=['name', 'RADeg', 'decDeg']
    for row in tab:
        objDict={}
        for key in wantedKeys:
            if key == 'name':
                objDict[key]=str(row[key])
            else:
                objDict[key]=row[key]
        catalog.append(objDict)
    
    return catalog

#-------------------------------------------------------------------------------------------------------------
def writePlot(objName, result, zPriorMax, plotsDir):
    """Write a .pdf plot of n(z), z using the given zCluster result dictionary.
    
    """

    if os.path.exists(plotsDir) == False:
        os.makedirs(plotsDir, exist_ok = True)
        
    if result != None:            
        
        # NOTE: in calculateRedshiftAndOdds, prior hasn't been applied to pz, but has been to zOdds
        pz=result['pz']
        zArray=result['pz_z']
        prior=np.ones(pz.shape)
        if zPriorMax != None:
            prior[np.greater(zArray, zPriorMax)]=0.0
            prior[np.less(zArray, 0.05)]=0.0
            pz=pz*prior
            norm=np.trapz(pz, zArray)
            pz=pz/norm   
        
        # Plot of normalised odds and p(z)        
        norm=np.trapz(result['zOdds'], result['pz_z'])
        result['zOdds']=result['zOdds']/norm
        
        plt.figure(figsize=(9, 5))

        ax=plt.axes([0.125, 0.125, 0.845, 0.79])
            
        plt.plot(result['pz_z'], result['zOdds'], 'k', label = '$n_{\Delta z}(z)$')
        plt.plot(result['pz_z'], pz, 'k:', label = '$n(z)$')

        plotMax=max([pz.max(), result['zOdds'].max()])
        plt.ylabel("$n_{\Delta z}(z)$ | $n(z)$ (normalised)")
        plt.xlabel("$z$")
        y=np.linspace(result['zOdds'].min(), plotMax*1.5, 4)
        plt.plot([result['z']]*len(y), y, 'k--')
        plt.ylim(0, plotMax*1.1)
        plt.xlim(0, zPriorMax*1.5)
        
        # Faffing with labels so we can lay out plots on a grid
        values, tickLabels=plt.yticks()
        for v, t in zip(values, tickLabels):
            t.set_text('%.1f' % (v))
        plt.yticks(values, tickLabels)
        
        leg=plt.legend(loc = 'upper right', numpoints = 1) 
        leg.draw_frame(False)
        plt.title("%s (z = %.2f)" % (objName.replace("_", " "), result['z'])) 
        plt.savefig(plotsDir+os.path.sep+"pz_"+objName.replace(" ", "_")+".pdf")
        plt.close()

#-------------------------------------------------------------------------------------------------------------
def writeRedshiftsCatalog(catalog, outFileName):
    """Writes a .fits table with cluster photo-zs in it
    
    """
    tab=atpy.Table()
    keys=['name', 'RADeg', 'decDeg', 'zcRADeg', 'zcDecDeg', 'offsetArcmin', 'offsetMpc', 'z', 'delta', 'errDelta']
    for key in keys:
        arr=[]
        for obj in catalog:
            arr.append(obj[key])
        tab.add_column(atpy.Column(arr, key))
    
    # Since generally we cross match against sourcery tables, we may as well zap the non-results as we don't need
    # The try... except bit here is because we've also used this routine to write the nullTest catalog, which doesn't have zs
    try:
        tab=tab[np.where(tab['z'] > 0.)]
    except:
        pass
    
    tab.table_name="zCluster"
    tab.write(outFileName, overwrite = True)

#-------------------------------------------------------------------------------------------------------------
def runOnCatalog(catalog, retriever, retrieverOptions, photoRedshiftEngine, outDir, zPriorMin,
                 zPriorMax, weightsType, maxRMpc, zMethod, bckCatalogFileName, bckAreaDeg2,
                 maskPath = None, writeGalaxyCatalogs = False, writeDensityMaps = False, writePlots = False, 
                 rank = 0, zDebias = None, fitZPOffsets = False):
    """Runs zCluster algorithm on each object in catalog.
    
    """
    
    # Now using sqlite database for storage, rather than loads of .pickle files (filesystem friendly)
    # We're only storing final results in here - if the user wants to see n(z) later, they can use -n option 
    # to rerun specific clusters
    # Each rank has it's own database to write into, and they all share a global database for reading
    conn=sqlite3.connect(outDir+os.path.sep+"redshifts_rank%d.db" % (rank))
    c=conn.cursor()
    c.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='redshifts'")
    matches=c.fetchall()
    if len(matches) == 0:
        c.execute("""CREATE TABLE redshifts (name text, RADeg real, decDeg real, zcRADeg real, zcDecDeg real, 
                  offsetArcmin real, offsetMpc real, z real, delta real, errDelta real)""")
    
    # The global database for checking if we've already measured a redshift - read only
    if os.path.exists(outDir+os.path.sep+"redshifts_global.db") == True:
        connGlobal=sqlite3.connect(outDir+os.path.sep+"redshifts_global.db")
        cGlobal=connGlobal.cursor()
    else:
        cGlobal=None
    
    # Optional area mask
    if maskPath is not None:
        with pyfits.open(maskPath) as img:
            maskMap=img[0].data
            maskWCS=astWCS.WCS(img[0].header, mode = 'pyfits')
    else:
        maskMap=None
        maskWCS=None
        
    # Optional background catalog (usually would only be used with proprietary photometric survey, e.g., SOAR)
    if bckCatalogFileName != None and bckAreaDeg2 != None:
        bckTab=atpy.Table().read(bckCatalogFileName)
        bckCatalog=catalogRetriever.parseFITSPhotoTable(bckTab, fieldIDKey = 'field', optionsDict = retrieverOptions)
        photoRedshiftEngine.calcPhotoRedshifts(bckCatalog, calcMLRedshiftAndOdds = True)
    else:
        bckCatalog=[]
    
    count=0
    for obj in catalog:
        count=count+1
        print(">>> %s (%d/%d):" % (obj['name'], count, len(catalog)))
        objOutDir=outDir+os.path.sep+obj['name']
        os.makedirs(objOutDir, exist_ok = True)
        
        # We actually query both the process database AND the global database
        c.execute("SELECT name FROM redshifts where name=?", (obj['name'],))
        matches=c.fetchall()
        if cGlobal != None:
            cGlobal.execute("SELECT name FROM redshifts where name=?", (obj['name'],))
            globalMatches=cGlobal.fetchall()
        else:
            globalMatches=[]
        if len(matches) == 0 and len(globalMatches) == 0:
            stuff="retry"
            while stuff == "retry":
                stuff=retriever(obj['RADeg'], obj['decDeg'], optionsDict = retrieverOptions)
            galaxyCatalog=stuff
            if galaxyCatalog is not None:
                
                if fitZPOffsets == True:
                    photoRedshiftEngine.calcZeroPointOffsets(galaxyCatalog)
                photoRedshiftEngine.calcPhotoRedshifts(galaxyCatalog, calcMLRedshiftAndOdds = True)                    
                    
                obj['zClusterResult']=clusterFinding.estimateClusterRedshift(obj['RADeg'], obj['decDeg'], galaxyCatalog, zPriorMin, 
                                                                             zPriorMax, weightsType, maxRMpc, zMethod, 
                                                                             maskMap = maskMap, maskWCS = maskWCS,
                                                                             bckCatalog = bckCatalog, bckAreaDeg2 = bckAreaDeg2,
                                                                             zDebias = zDebias)
                
                if obj['zClusterResult'] is not None:
                    dMapDict=clusterFinding.makeDensityMap(obj['RADeg'], obj['decDeg'], galaxyCatalog, 
                                                              obj['zClusterResult']['z'], dz = 0.1)
                    obj['zcRADeg']=dMapDict['cRADeg']
                    obj['zcDecDeg']=dMapDict['cDecDeg']
                    obj['offsetArcmin']=dMapDict['offsetArcmin']
                    obj['offsetMpc']=dMapDict['offsetMpc']
                    if writeDensityMaps == True:
                        astImages.saveFITS(objOutDir+os.path.sep+"densityMap_%s.fits" % (obj['name'].replace(" ", "_")), 
                                           dMapDict['map'], dMapDict['wcs']) 
                        # Useful for debugging, but NOT if we specified a pre-made mask
                        if maskPath is None:    
                            astImages.saveFITS(objOutDir+os.path.sep+"areaMap_%s.fits" % (obj['name'].replace(" ", "_")), 
                                               obj['zClusterResult']['areaMask'], obj['zClusterResult']['wcs'])
                else:
                    obj['zcRADeg']=-99
                    obj['zcDecDeg']=-99
                    obj['offsetArcmin']=-99
                    obj['offsetMpc']=-99
                
                # This is just to stop pickle-related fail if under MPI
                # Really we should tidy up the area mask making stuff and call before estimateClusterRedshift
                if obj['zClusterResult'] is not None:
                    if 'wcs' in obj['zClusterResult'].keys():
                        del obj['zClusterResult']['wcs']
                    if 'areaMask' in obj['zClusterResult'].keys():
                        del obj['zClusterResult']['areaMask']

                # Write out galaxy catalog with photo-zs in both FITS and DS9 .reg format
                if writeGalaxyCatalogs == True:
                    # Also r-i colour, useful for sanity checking of BCGs
                    for gobj in galaxyCatalog:
                        if 'r' in list(gobj.keys()) and 'i' in list(gobj.keys()):
                            gobj['r-i']=gobj['r']-gobj['i']
                        if 'r' in list(gobj.keys()) and 'z' in list(gobj.keys()):
                            gobj['r-z']=gobj['r']-gobj['z']

                    wantedKeys=['id', 'RADeg', 'decDeg', 'u', 'uErr', 'g', 'gErr', 'r', 'rErr', 'i', 'iErr', 'z', 'zErr', 'Ks', 'KsErr', 'zPhot', 'odds', 'r-i', 'r-z']
                    if os.path.exists(objOutDir+os.path.sep+"galaxyCatalog_%s.fits" % (obj['name'].replace(" ", "_"))) == True:
                        os.remove(objOutDir+os.path.sep+"galaxyCatalog_%s.fits" % (obj['name'].replace(" ", "_")))
                    tab=atpy.Table()
                    for key in wantedKeys:
                        arr=[]
                        for gobj in galaxyCatalog:
                            try:
                                arr.append(gobj[key])
                            except:
                                arr.append(99)
                                #print "missing key, eh?"
                                #IPython.embed()
                                #sys.exit()
                        tab.add_column(atpy.Column(np.array(arr), key))
                    # NOTE: to cut down on disk space this takes, include only galaxies within some radius
                    # Chosen one is just over 1.5 Mpc at z = 0.1
                    tab.add_column(atpy.Column(astCoords.calcAngSepDeg(obj['RADeg'], obj['decDeg'], 
                                                                    np.array(tab['RADeg']), np.array(tab['decDeg'])), "rDeg"))
                    #tab=tab[np.where(tab['rDeg'] < 14./60.0)]
                    tab.table_name="zCluster"
                    tab.write(objOutDir+os.path.sep+"galaxyCatalog_%s.fits" % (obj['name'].replace(" ", "_")))
                    try:
                        catalogTools.catalog2DS9(galaxyCatalog, 
                                                 objOutDir+os.path.sep+"galaxyCatalog_%s.reg" % (obj['name'].replace(" ", "_")), 
                                                 idKeyToUse = 'id', 
                                                 addInfo = [{'key': 'r', 'fmt': '%.3f'}, {'key': 'zPhot', 'fmt': '%.2f'}]) 
                    except:
                        print("... couldn't write .reg file (missing key?) ...")
                        pass

                if writePlots == True:
                    writePlot(obj['name'], obj['zClusterResult'], zPriorMax, outDir+os.path.sep+"Plots")
                
            else:
                obj['zClusterResult']=None

            # Insert a row of data
            if obj['zClusterResult'] is not None:
                c.execute("INSERT INTO redshifts VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)", 
                      (obj['name'], obj['RADeg'], obj['decDeg'], obj['zcRADeg'], obj['zcDecDeg'], obj['offsetArcmin'],
                       obj['offsetMpc'], obj['zClusterResult']['z'], obj['zClusterResult']['delta'], 
                       obj['zClusterResult']['errDelta']))
                obj['z']=obj['zClusterResult']['z']
                obj['delta']=obj['zClusterResult']['delta']
                obj['errDelta']=obj['zClusterResult']['errDelta']
            else:
                c.execute("INSERT INTO redshifts VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)", 
                      (obj['name'], obj['RADeg'], obj['decDeg'], -99, -99, -99, -99, -99, 0, -99))
                obj['z']=-99
                obj['delta']=0
                obj['errDelta']=0
            conn.commit() # May want to think about how often we do this...
            
        else:
            print("... already measured photo-z ...")
            # Prefer the global result, if it exists
            if len(matches) > 0:
                thisCursor=c
            if len(globalMatches) > 0:
                thisCursor=cGlobal
            thisCursor.execute("SELECT z, delta, errDelta, RADeg, decDeg, offsetArcmin, offsetMpc FROM redshifts WHERE name = ?", (obj['name'],))
            match=thisCursor.fetchone()
            obj['z']=match[0]
            obj['delta']=match[1]
            obj['errDelta']=match[2]
            obj['zcRADeg']=match[3]
            obj['zcDecDeg']=match[4]
            obj['offsetArcmin']=match[5]
            obj['offsetMpc']=match[6]
 
    return catalog

#-------------------------------------------------------------------------------------------------------------
def getRoughFootprint(database):
    """Returns RAMin, RAMax, decMin, decMax for given photometric survey indicated by database. Used for
    null tests.
    
    """

    if database in ['SDSSDR7', 'SDSSDR8', 'SDSSDR10', 'SDSSDR12']:
        RAMin, RAMax, decMin, decMax=[130.0, 250.0, 0.0, 60.0]
    elif database == 'S82':
        RAMin, RAMax, decMin, decMax=[-60.0, 60.0, -1.2, 1.2]
    elif database == 'CFHTLenS':
        RAMin, RAMax, decMin, decMax=[30.5, 38.5, -11.5, -2.5]
    else:
        print("WARNING: no rough footprint defined for database '%s' yet" % (database))
        return None
    
    return RAMin, RAMax, decMin, decMax
    
#-------------------------------------------------------------------------------------------------------------
if __name__ == '__main__':

    parser = argparse.ArgumentParser("zCluster")
    parser.add_argument("catalogFileName", help="""A .fits table or a .csv file with columns 'name', 'RADeg',
                        'decDeg'. Set to 'nullTest', to create a catalog with 1000 random positions
                        that are more than 5 arcmin away from redMaPPer and NED clusters.""")
    parser.add_argument("database", help="""The photometric database to use. Options are 'SDSSDR12', 'S82' 
                        (for SDSS DR7 Stripe 82 co-add); 'CFHTLenS'; 'DESDR1' [experimental], 
                        'DESY3' [experimental; requires access to proprietary DES data]; 'PS1' [experimental]; 
                        'DECaLS' (DR8) [experimental]; or the path to a .fits table with columns in the 
                        appropriate format ('ID', 'RADeg', 'decDeg', and magnitude column names in the form 
                        'u_MAG_AUTO', 'u_MAGERR_AUTO' etc.).""")
    parser.add_argument("-o", "--output-label", dest="outLabel", help="""Label to use for outputs  
                        (default: catalogFileName_database, stripped of file extension). 
                        A redshift catalog called zCluster_outLabel.fits will be created. Cached
                        results for each entry in the input cluster catalog and associated plots will
                        be written into the directory outLabel/, together with a log file that
                        records the arguments used for running zCluster.""", default = None)
    parser.add_argument("-w", "--weights-type", dest="weightsType", help="""Radial weighting type. Options
                        are 'NFW', 'flat', or 'radial' (default: NFW).""", default = 'NFW')
    parser.add_argument("-R", "--max-radius-Mpc", dest="maxRMpc", help="""Maximum radius in Mpc within 
                        which to calculate delta statistic for each cluster (default: 0.5).""", 
                        default = 0.5)
    parser.add_argument("-m", "--mask", dest="mask", help="""Path to mask image (FITS format with WCS)
                        that defines valid survey area.""", default = None)
    parser.add_argument("-a", "--algorithm", dest="algorithm", help="""Algorithm to use for the maximum likelihood
                        redshift. Options are 'max' or 'odds' (default: odds).""", 
                        default = 'odds')
    parser.add_argument("-c", "--cachedir", dest="cacheDir", default = None, help="""Cache directory location
                        (default: $HOME/.zCluster/cache). Downloaded photometric catalogs will be stored 
                        here.""")
    parser.add_argument("-M", "--mpi", dest="MPIEnabled", action="store_true", help="""Enable MPI. If you
                        want to use this, run zCluster using something like: mpirun --np 4 zCluster ...""", 
                        default = False)
    parser.add_argument("-e", "--max-mag-error", dest="maxMagError", help="""Maximum acceptable 
                        photometric error (in magnitudes; default: 0.25).""", default = 0.25)
    parser.add_argument("-E", "--photometric-zero-point-error", dest="ZPError", type = float, 
                        help="""Global photometric zero point uncertainty in magnitudes, applied to all bands 
                        (default: 0). Added in quadrature to the photometric uncertainties in the catalog.""", 
                        default = 0.0)
    parser.add_argument("-f", "--fit-for-zero-point-offsets", dest="fitZPOffsets", 
                        help="""If the input catalog contains a z_spec column, use those galaxies to fit
                        for magnitude zero point offsets. These will then be applied when estimating galaxy
                        photometric redshifts.""", 
                        default = False, action = "store_true")
    parser.add_argument("-z", "--z-prior-min", dest="zPriorMin", help="""Set minimum redshift of prior.""", 
                        default = None)
    parser.add_argument("-Z", "--z-prior-max", dest="zPriorMax", help="""Set maximum redshift of prior.""", 
                        default = None)
    parser.add_argument("-b", "--brighter-absmag-cut", dest="absMagCut", help="""Set bright absolute magnitude cut.""", 
                        default = -24.)
    parser.add_argument("-n", "--name", dest="name", help="Find photo-z of only the named cluster in the catalog.")
    parser.add_argument("-t", "--templates-directory", dest="templatesDir", help="""Specify a directory containing
                        a custom set of spectral templates.""", 
                        default = None)
    parser.add_argument("-d", "--write-density-maps", dest="writeDensityMaps", action="store_true", 
                        help="""Write out a .fits image projected density map (within delta z = +/- 0.1 of the best 
                        fit redshift) for each cluster.""",
                        default = False)
    parser.add_argument("-W", "--write-galaxy-catalogs", dest="writeGalaxyCatalogs", action="store_true", 
                        help="""Write out a .fits format galaxy catalog and DS9 .reg file for each cluster.""", 
                        default = False)
    parser.add_argument("-P", "--write-plots", dest="writePlots", action="store_true", 
                        help = """Write out a .pdf plot of n(z) for each cluster.""", default = False)
    parser.add_argument("-s", "--add-SDSS", dest="addSDSS", action="store_true", help="""If using 
                        a user-supplied FITS galaxy photometic catalog, add in additional SDSS 
                        photometry if available.""", default = False)
    parser.add_argument("-B", "--background-catalog", dest="bckCatalogFileName", help="""A .fits table with columns 
                        in the appropriate format ('ID', 'RADeg', 'decDeg', and magnitude column names in the form 
                        'u_MAG_AUTO', 'u_MAGERR_AUTO' etc.) to be used as the background sample for delta estimates.
                        If this is given, the area covered by the background catalog must be given also (-A flag)""")
    parser.add_argument("-A", "--background-area-deg2", dest="bckAreaDeg2", default = None, help="""The area, 
                        in square degrees, covered by the background galaxy catalog given using the -B flag.""")
    parser.add_argument("-C", "--credentials-filename", dest="credentialsFileName", default = None, 
                        help = """The location of a file containing username (first line), password (second line), 
                        for login to e.g., ESO Portal (this option is only currently used for the KIDSDR3
                        database option)""")

    args = parser.parse_args()

    catalogFileName=args.catalogFileName
    database=args.database
    outLabel=args.outLabel
    cacheDir=args.cacheDir
    weightsType=args.weightsType
    maxRMpc=float(args.maxRMpc)
    method=args.algorithm
    MPIEnabled=args.MPIEnabled
    maxMagError=float(args.maxMagError)
    #magsBrighterMStarCut=float(args.magsBrighterMStarCut)
    absMagCut=float(args.absMagCut)
    writeGalaxyCatalogs=args.writeGalaxyCatalogs
    writePlots=args.writePlots
    maskPath=args.mask
    ZPError=args.ZPError
    
    update_rcParams()

    if outLabel == None:
        baseOutputLabel=os.path.split(catalogFileName)[-1]
        baseOutputLabel=baseOutputLabel.replace(".fits", "").replace(".csv", "")
        baseOutputLabel=baseOutputLabel+"_%s" % (database.replace(".fits", ""))
    else:
        baseOutputLabel=outLabel
        
    outDir=baseOutputLabel
    outFileName="zCluster_"+baseOutputLabel+".fits"

    if method not in ['odds', 'max']:
        raise Exception("method must be 'odds' or 'max'")
    
    if weightsType not in ['flat', 'radial', 'NFW']:
        raise Exception("weights must be 'flat', 'radial', or 'NFW'")

    # These are ONLY used if not None...
    bckCatalogFileName=args.bckCatalogFileName
    if args.bckAreaDeg2 == None and bckCatalogFileName != None:
        raise Exception("area covered by separate background galaxy catalogue must be given.")
    if args.bckAreaDeg2 != None:
        bckAreaDeg2=float(args.bckAreaDeg2)
    else:
        bckAreaDeg2=None
    
    if MPIEnabled ==True:
        from mpi4py import MPI
        comm=MPI.COMM_WORLD
        size=comm.Get_size()
        rank=comm.Get_rank()
        if size == 1:
            raise Exception("if you want to use MPI, run with mpirun --np 4 zCluster ...")
    else:
        rank=0

    if rank == 0:
        if cacheDir is not None:
            if os.path.exists(cacheDir) == False:
                os.makedirs(cacheDir, exist_ok = True)
        else:
            if os.path.exists(catalogRetriever.CACHE_DIR) == False:
                os.makedirs(catalogRetriever.CACHE_DIR, exist_ok = True)
                    
    # Default prior cuts are defined for each database here... 
    # Can be overridden with args.zPriorMin, args.zPriorMax... see below
    retrieverOptions={}
    passbandSet='SDSS+Ks'
    zDebias=None    # A fudge used to correct output zs for some surveys (final z = z + zDebias*(1+z))
    ZPOffsets=None
    if database == 'S82':
        retriever=catalogRetriever.S82Retriever
        zPriorMin=0.20
        zPriorMax=1.5
        retrieverOptions={'maxMagError': maxMagError}
    elif database == 'SDSSDR7':
        retriever=catalogRetriever.SDSSDR7Retriever
    elif database == 'SDSSDR8':
        retriever=catalogRetriever.SDSSDR8Retriever
    elif database == 'SDSSDR10':
        retriever=catalogRetriever.SDSSDR10Retriever
    elif database == 'SDSSDR12':
        retriever=catalogRetriever.SDSSDR12Retriever
        zPriorMin=0.05
        zPriorMax=0.8
        retrieverOptions={'maxMagError': maxMagError}
    elif database == 'PS1':
        import mastcasjobs
        if not os.environ.get('CASJOBS_WSID'):
            raise Exception("Set CASJOBS_WSID environment variable to use PS1 (see: http://ps1images.stsci.edu/ps1_dr2_query.html - to get your WSID, go to http://mastweb.stsci.edu/ps1casjobs/ChangeDetails.aspx and check your 'profile' page)")
        if not os.environ.get('CASJOBS_PW'):
            raise Exception("Set CASJOBS_PW environment variable to use PS1 (see: http://ps1images.stsci.edu/ps1_dr2_query.html)")
        retriever=catalogRetriever.PS1Retriever
        # Min prior here set to match 4 Mpc max search radius
        zPriorMin=0.15
        zPriorMax=0.6
        passbandSet='PS1'
        retrieverOptions={'maxMagError': maxMagError, 'jobs': mastcasjobs.MastCasJobs(context="PanSTARRS_DR2")}
    elif database == 'DESY3':
        import easyaccess as ea
        connection=ea.connect(section = 'dessci')
        #connection=None
        retriever=catalogRetriever.DESY3Retriever
        zPriorMin=0.05
        zPriorMax=1.5
        passbandSet='DES'
        retrieverOptions={'maxMagError': maxMagError, 'connection': connection}
        zDebias=0.02    # From testing against spec-zs (don't know yet why there is a mean offset)
    elif database == 'DESY3+WISE':
        import easyaccess as ea
        connection=ea.connect(section = 'dessci')
        retriever=catalogRetriever.DESY3WISERetriever
        zPriorMin=0.05
        zPriorMax=1.5
        passbandSet='DES+WISE'
        retrieverOptions={'maxMagError': maxMagError, 'connection': connection}
        zDebias=0.02    # From testing against spec-zs (don't know yet why there is a mean offset)
    elif database == 'DESDR1':
        import easyaccess as ea
        connection=ea.connect(section = 'desdr')
        retriever=catalogRetriever.DESDR1Retriever
        zPriorMin=0.05
        zPriorMax=1.5
        passbandSet='DES'
        retrieverOptions={'maxMagError': maxMagError, 'connection': connection}
    elif database == 'KiDSDR4':
        retriever=catalogRetriever.KiDSDR4Retriever
        zPriorMin=0.05
        zPriorMax=1.5
        passbandSet='KiDS-VIKING'
        retrieverOptions={'maxMagError': maxMagError}
    elif database == 'ATLASDR4':
        retriever=catalogRetriever.ATLASDR4Retriever
        zPriorMin=0.05
        zPriorMax=0.8
        passbandSet='KiDS-VIKING'
        retrieverOptions={'maxMagError': maxMagError}
    elif database == 'CFHTLenS':
        retriever=catalogRetriever.CFHTLenSRetriever
        zPriorMin=0.05
        zPriorMax=1.5
        retrieverOptions={'maxMagError': maxMagError}
    elif database == 'DECaLS':
        # For DECaLS, need the bricks files that define survey on the sky
        # These were previously included in zCluster, but now we fetch over web and cache
        bricksCacheDir=catalogRetriever.CACHE_DIR
        catalogRetriever.makeCacheDir()
        bricksPath=bricksCacheDir+os.path.sep+"survey-bricks.fits.gz"
        if os.path.exists(bricksPath) == False:
            print("... fetching and caching DECaLS survey-bricks.fits.gz ...")
            urllib.request.urlretrieve("http://portal.nersc.gov/project/cosmo/data/legacysurvey/dr8/survey-bricks.fits.gz", bricksPath)
        bricksDR8Path=bricksCacheDir+os.path.sep+"survey-bricks-dr8-south.fits.gz"
        if os.path.exists(bricksDR8Path) == False:
            print("... fetching and caching DECaLS survey-bricks-dr8-south.fits.gz ...")
            urllib.request.urlretrieve("https://portal.nersc.gov/project/cosmo/data/legacysurvey/dr8/south/survey-bricks-dr8-south.fits.gz", bricksDR8Path)
        bricksTab=atpy.Table().read(bricksPath)
        DR8Tab=atpy.Table().read(bricksDR8Path)
        DR8Tab.rename_column("brickname", "BRICKNAME")
        retriever=catalogRetriever.DECaLSRetriever
        zPriorMin=0.05
        zPriorMax=1.5
        passbandSet='DECaLS'
        # Zero point offsets had no effect on correcting the bias
        #ZPOffsets=np.array([-0.00878495, -0.019883, 0.01998477, -0.03047624, 0.00860256])
        #ZPOffsets=np.array([-0.00380799, -0.01845335, 0.01706214, -0.0291939, 0.01454793])
        #ZPOffsets=np.array([-0.07441184, -0.04284442, 0.01009525, 0.01008043, 0.05755784]) 
        ZPOffsets=np.array([0.02271222, -0.05051711, -0.02465597, -0.00406835, 0.05406105])
        zDebias=0.02    # From testing against spec-zs (don't know yet why there is a mean offset)
        retrieverOptions={'maxMagError': maxMagError, 'bricksTab': bricksTab, 'DR8Tab': DR8Tab}
    elif database == 'CFHTDeep':
        retriever=catalogRetriever.CFHTDeepRetriever
    elif database == 'CFHTWide':
        retriever=catalogRetriever.CFHTWideRetriever
        zPriorMin=0.05
        zPriorMax=1.5
    else:
        # Assume this is a FITS file in format of ACAM or SOI photometric pipelines
        # NOTE: SOI has 5' field of view, and targets have no z from SDSS anyway, so zPriorMin = 0.5 
        # 'addSDSS' option here queries SDSS, cross matches, and adds e.g. g-band mags if we don't have them
        zPriorMin=0.01
        zPriorMax=2.0
        passbandSet='WIRDS'
        retriever=catalogRetriever.FITSRetriever
        retrieverOptions={'fileName': database, 'addSDSS': args.addSDSS, 'maxMagError': maxMagError}

    # Allow prior overrides from command line
    if args.zPriorMax != None:
        zPriorMax=float(args.zPriorMax)
    if args.zPriorMin != None:
        zPriorMin=float(args.zPriorMin)

    # Set-up output dir and log the options we're running with
    if rank == 0:
        if os.path.exists(outDir) == False:
            os.makedirs(outDir, exist_ok = True)
        if args.name != None:
            logDir=logFileName=outDir+os.path.sep+args.name
            if os.path.exists(logDir) == False:
                os.makedirs(logDir, exist_ok = True)
            logFileName=outDir+os.path.sep+args.name+os.path.sep+"zCluster.log"
        else:
            logFileName=outDir+os.path.sep+"zCluster.log"
        logFile=open(logFileName, "w")
        logFile.write("started: %s\n" % (datetime.datetime.now().isoformat()))
        for key in list(args.__dict__.keys()): 
            if key not in ['zPriorMin', 'zPriorMax']:
                logFile.write("%s: %s\n" % (key, str(args.__dict__[key])))
        logFile.write("zPriorMin: %.3f\n" % (zPriorMin))
        logFile.write("zPriorMax: %.3f\n" % (zPriorMax))
        logFile.close()
        
    if cacheDir != None:
        retrieverOptions['altCacheDir']=cacheDir
            
    photoRedshiftEngine=PhotoRedshiftEngine.PhotoRedshiftEngine(absMagCut, passbandSet = passbandSet, 
                                                                ZPError = ZPError, ZPOffsets = ZPOffsets,
                                                                templatesDir = args.templatesDir)
        
    # Either make a null test catalog, or parse the one we asked for
    runningNullTest=False
    if catalogFileName == 'nullTest':
        runningNullTest=True
        # Generate a catalog of random points
        print(">>> Running null test ...")
        catalogFileName="nullTest.fits"
        if os.path.exists(catalogFileName) == False:
            rmTab=atpy.Table().read(zCluster.__path__[0]+os.path.sep+"data"+os.path.sep+"redmapper_dr8_public_v5.10_catalog.fits")
            RAMin, RAMax, decMin, decMax=getRoughFootprint(database)
            catalog=[]
            count=0
            while count < 1000:
                obj={}
                obj['name']='null%d' % (count)
                obj['RADeg']=np.random.uniform(RAMin, RAMax)
                obj['decDeg']=np.random.uniform(decMin, decMax)
                if obj['RADeg'] < 0:
                    obj['RADeg']=360.0+obj['RADeg']
                NEDInfo=catalogTools.getNEDInfo(obj, crossMatchRadiusDeg = 5.0/60.0, refetch = True)
                rDeg=astCoords.calcAngSepDeg(obj['RADeg'], obj['decDeg'], np.array(rmTab['RA']), np.array(rmTab['DEC']))
                if NEDInfo == None and (rDeg.min()*60.0) > 5.0:                    
                    catalog.append(obj)
                    count=count+1
                else:
                    print("... rejected position near NED or RM cluster ...")
            writeRedshiftsCatalog(catalog, catalogFileName)
        else:
            print("... using existing nullTest.fits catalog...")
            catalog=parseClusterCatalog(catalogFileName)
            #catalog=catalog[:500] # For testing   
    else:
        catalog=parseClusterCatalog(catalogFileName)
    
    # Optionally only running on a single cluster
    if args.name != None:
        foundObj=False
        for objDict in catalog:
            if objDict['name'] == args.name:
                foundObj=True
                break
        catalog=[objDict]
        if foundObj == False:
            raise Exception("didn't find %s in input catalog" % (args.name))

    if MPIEnabled == True:
        # New - bit clunky but distributes more evenly
        rankCatalogs={}
        rankCounter=0
        for objDict in catalog:
            if rankCounter not in rankCatalogs:
                rankCatalogs[rankCounter]=[]
            rankCatalogs[rankCounter].append(objDict)
            rankCounter=rankCounter+1
            if rankCounter > size-1:
                rankCounter=0
        if rank in rankCatalogs.keys():
            catalog=rankCatalogs[rank]
        else:
            catalog=[]

    catalog=runOnCatalog(catalog, retriever, retrieverOptions, photoRedshiftEngine, outDir,
                         zPriorMin, zPriorMax, weightsType, maxRMpc, method, bckCatalogFileName, bckAreaDeg2,
                         maskPath = maskPath, writeGalaxyCatalogs = writeGalaxyCatalogs, 
                         writeDensityMaps = args.writeDensityMaps, writePlots = writePlots, 
                         rank = rank, zDebias = zDebias, fitZPOffsets = args.fitZPOffsets)
    
    # If running under MPI, gather everything back together
    # Rank 0 process will continue with plots
    if MPIEnabled == True:
        if rank == 0:
            wholeCatalog=catalog
            for i in range(1, size):
                catalogPart=comm.recv(source = i, tag = 11)
                wholeCatalog=wholeCatalog+catalogPart
            catalog=wholeCatalog
        else:
            comm.send(catalog, dest = 0, tag = 11)
            sys.exit()
                
    # Makes a global .db file (might be useful) after run complete
    if os.path.exists(outDir+os.path.sep+"redshifts_global.db") == True:
        os.remove(outDir+os.path.sep+"redshifts_global.db")
    print(">>> Updating global redshifts .db file ...")
    conn=sqlite3.connect(outDir+os.path.sep+"redshifts_global.db")
    c=conn.cursor()
    c.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='redshifts'")
    matches=c.fetchall()
    if len(matches) == 0:
        c.execute("""CREATE TABLE redshifts (name text, RADeg real, decDeg real, zcRADeg real, zcDecDeg real,
                  offsetArcmin real, offsetMpc real, z real, delta real, errDelta real)""")
    for obj in catalog:
        try:
            c.execute("INSERT INTO redshifts VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)", 
                      (obj['name'], obj['RADeg'], obj['decDeg'], obj['zcRADeg'], obj['zcDecDeg'], obj['offsetArcmin'],
                       obj['offsetMpc'], obj['z'], obj['delta'], obj['errDelta']))
        except:
            raise Exception("Failed to insert %s (keys: %s)" % (obj['name'], obj.keys()))
    conn.commit()
    conn.close()
        
    if runningNullTest == True:
        deltaArr=[]
        for obj in catalog:
            if 'zClusterResult' in list(obj.keys()) and obj['zClusterResult'] != None:
                if 'delta' in list(obj['zClusterResult'].keys()):
                    deltaArr.append(obj['zClusterResult']['delta'])
                elif 'SNR' in list(obj['zClusterResult'].keys()):
                    deltaArr.append(obj['zClusterResult']['SNR'])
            else:
                deltaArr.append(0)
        deltaArr=np.array(deltaArr)
        #deltaArr[np.less(deltaArr, 0)]=0    # or should we allow -ve?
        deltaArr.sort()
        cumulativeRange=np.linspace(0, 100, 401)
        cumulativeArr=[]
        for c in cumulativeRange:
            count=np.greater(deltaArr, c).sum()
            cumulativeArr.append(count)
        cumulativeArr=np.array(cumulativeArr, dtype = float)
        percentileArr=(cumulativeArr/deltaArr.shape[0])*100
        
        update_rcParams()
        plt.figure(figsize=(9,6.5))
        #fontDict={'size': 18, 'family': 'serif'}
        ax=plt.axes([0.10, 0.11, 0.88, 0.87])
        #plt.tick_params(axis='both', which='major', labelsize=15)
        #plt.tick_params(axis='both', which='minor', labelsize=15)
        plt.plot(cumulativeRange, percentileArr, 'k', lw = 2)
        plotMax=100
        plt.ylabel("Null test detections (% > $\delta$)")#, fontdict = fontDict)
        plt.xlabel("$\delta$")#, fontdict = fontDict)
        plt.ylim(0, percentileArr[np.greater(cumulativeRange, 1)].max()*1.1)
        plt.xlim(1, cumulativeRange[np.where(percentileArr == 0)[0][0]]+2)
        plt.savefig(outDir+os.path.sep+"nullTest.pdf")
        plt.close()

        print(">>> Null test stats:")
        for n in range(2, 9):
            print("... percentage false detections at delta > %d = %.2f" % (n, percentileArr[np.equal(cumulativeRange, n)][0]))

    # Want this either way
    writeRedshiftsCatalog(catalog, outFileName)
    
        
